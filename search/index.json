[{"content":"はじめに この記事は，「長野高専 Advent Calendar 2022」19日目に合わせて書かれた記事です． 16日目にも記事を書いたやじるしです．16日目の記事の最後に19日目にも記事を書くと言いました．書きました．僕は音声合成がユーザー側の視点で好きです．好きだとどうなるのか．作ってみたくなります．ということで，前回に書いたベイズ機械学習の知見を使って，統計的音声合成とくに隠れマルコフモデルによる音声合成について仕組みを自分自身で噛み砕きながら理解していきたいと思います．\n音声合成とは そもそも，音声合成とはどのようなものなのでしょうか． Wikipediaの音声合成を引用します．\n ヒトは発声器官を通じて音声を生成し、コミュニケーションを行なう（会話や演説、講演、各種芸能およびその中継や録音・録画など）。この音声を人工的に生成するタスクが音声合成である。合成された音声を合成音声と呼ぶ。\n 音声合成は，このコミュニケーションをとるためのインターフェースである音声を機械（コンピュータ）によって人工的に再現しようという試みです． 音声合成によって得られた合成音声は今の社会で様々な場所で使われています． 今もしかしたら聞いているかもしれないボカロ曲は，歌声音声合成エンジンによって合成された歌声，実況動画や解説動画で聞こえてくる声も合成音声かもしれません．このようなエンターテイメントによる利用だけでなく，例えば，電車に乗っている間に聞こえてくる乗換情報のアナウンス，災害情報を瞬時に伝える全国瞬時警報システム（Jアラート），自分のスマートフォンの中にいるアシスタントなども合成された音声によって私達の耳に届いて，大切な情報を正確に伝えてくれています．合成音声は遠い存在ではなく，私達の生活の中に溶け込んでいる身近な存在なのです．さらに，コンピュータと音声を通してコミュニケーションを取る，医療現場などに応用するなど，他の技術と組み合わせることで，有用なシステムを作り出すことができます．今回はこのような面白い技術について，特に仕組みに注目して書いていきます．\n音声合成の手法について 音声合成を行う手法としては，これまでの研究から様々なものが考案されており， 大きく分けて，「規則合成方式」，「素片選択型方式」，「統計的パラメトリック音声合成方式」，「一貫学習方式」 が挙げられます．\n 規則合成方式 : 韻律やフォルマントの規則を記述する方式．足立レイはフォルマント合成なのでこの方式に入るはず． 素片選択型方式 : 音声素片（音素）を接続することで音声を合成する方式．UTAU，VOCALOID[1]，VOICEROIDなど． 統計的パラメトリック音声合成 : 生成モデルによってテキストから音声への生成過程を記述し合成する方式．CeVIO，Open JTalk，Sinsy，A.I.VOICE[2]，Synthesizer Vなど． 一貫学習方式 : 音声特徴量，言語特徴量の解析による前処理，音響モデルによる推定，合成を一貫して一つのモデルで行う方式．Tacotronなど．  挙げた例の中には，複数の方式を使って音声を合成するハイブリッドな方式を用いているものもあります．\n統計的パラメトリック音声合成 統計的パラメトリック音声合成は，音声とテキストの関係が何かしらの確率分布に従っているという仮定のもとで，生成モデルによってそれを記述する手法です．生成モデルは，生成過程を考えることでモデリングされた同時分布によるモデルで，識別モデルのような条件付き確率分布と違い，確率モデルからサンプリングすることで特徴量の生成を行うことができます．\nでは，統計的パラメトリック音声合成はどのように音声合成を行うのでしょうか． 最終的な目標は，与えられたテキストから何かしらの処理をして音声を生成することです． 音声の合成の流れを次の図に示します．はじめに，テキスト$s$から言語特徴量を取り出します．この言語特徴量から，確率モデルで記述された音響モデルに入れることにより，このテキストに沿った音声特徴量$\\boldsymbol{Wx}$を得ることができます．この得られた音声特徴量をボコーダによって合成することにより波形が生成され，合成された音声が得られます．\n生成モデルとして記述するのは，音響特徴量を予測する音響モデルの部分です．音響モデルはテキストから得られる言語特徴量から，音声波形の生成に必要な音声特徴量を計算する，合成音声の品質に大きく影響するモジュールとなります．音響モデルには，大きく隠れマルコフモデル(HMM)によるモデルと深層ニューラルネットワーク(DNN)によるモデルが挙げられます．\n HMM音声合成  少ないデータで構築可能．声色を柔軟に制御可能．サイズが小さい． 状態系列（音素アラインメント）も同時に学習される． 素片選択型より品質が劣化する．   DNN音声合成  言語と音声の複雑な関係を高制度にモデル化可能なので，高品質な音声が得られる． 言語特徴量と音声特徴量の系列が異なるため，モデルによっては継続帳予測，音声特徴量予測を行う2つのモデルを作る必要がある．    HMM音声合成，DNN音声合成どちらも利点と弱点があることがわかります． どちらの手法も，音響モデルをモデリングして，言語特徴量から音響特徴量を得るという根本的な考え方は同じです．そのため，どちらかの手法について仕組みを理解すれば，もう一つの手法はただモデルを置き換えただけと考えることができると思います．\n今回は，隠れマルコフモデル(HMM)による音声合成について，どのようにモデリングして学習を行うのかをベイズ的アプローチに基づいて行っているものを調べてみます．論文は「ベイズ的アプローチに基づくHMM音声合成」を参考にし，隠れマルコフモデルのモデリングと変分推論で用いる変分下限の計算までの部分について，実際に式の展開やグラフィカルモデルを用いながら理解していきたいと思います．言語特徴量の抽出部と，音声波形の生成を行うボコーダのことは今回は書いていません．（ボコーダにも，WORLDにような古典的なアルゴリズムを複数用いたものや，WaveNetのような深層学習を応用したものまで様々な手法があります．たしかに，音響特徴量から波形を生成するところでも音声の精度は変わりそうです．）\n問題設定 隠れマルコフモデルを用いた音声合成について，問題を定めて見通しを立てます． 隠れマルコフモデルによる音響モデルからは，動的特徴量を含む音声パラメータ$\\boldsymbol{Wx}$を合成したいです．ここで，$\\boldsymbol{x}$が音声パラメータの静的特徴量であり，以下の式で書けます． $$ \\boldsymbol{x} = (\\boldsymbol{x}_1^\\top, \\boldsymbol{x}_2^\\top, \\cdots, \\boldsymbol{x} _{T_x}^\\top)^\\top $$\n$\\boldsymbol{W}$は，静的特徴量から動的特徴量に変換する行列であり，$\\boldsymbol{x}_t$から動的特徴量に変換した行列$(\\boldsymbol{Wx})_t$は次の式で書けます $$ (\\boldsymbol{Wx})_t = (\\boldsymbol{x}_t^\\top, \\Delta\\boldsymbol{x}_t^\\top, \\Delta^2\\boldsymbol{x}_t^\\top)^\\top $$\n動的特徴量は静的特徴量の変化である一階微分，二階微分を表した量であり，これを音声特徴量に導入することで，状態遷移部やモデル接続部で歪みの少ないなめらかな音声を生成できるようです．\nモデル構築 はじめに，音声合成を行う隠れマルコフモデルを定義します． 音声特徴量の観測ベクトル列$\\boldsymbol{O}$を$\\boldsymbol{O} = (\\boldsymbol{o}_1, \\boldsymbol{o}_2, \\cdots, \\boldsymbol{o}_T)$，状態遷移を表す隠れ変数列(状態系列)$\\boldsymbol{Z}$を$\\boldsymbol{Z} = (z_1, z_2, \\cdots, z_T), z_t \\in \\lbrace1, \\cdots N\\rbrace$とします．ただし，$T$は出力する音声のサンプリング数，$N$は隠れマルコフモデルの状態数を表します．グラフィカルモデルを図に示します．\nここで，議論を行いやすくするために$z_t^i$を式(1)で定義します． $$ \\begin{equation} z_t^i = \\begin{cases} 1 \\ (z_t = i) \\\\ 0 \\ (\\text{otherwise}) \\end{cases} \\end{equation} $$\n隠れマルコフモデルが持つパラメータ$\\boldsymbol{\\Lambda} = \\lbrace \\boldsymbol{a}, \\boldsymbol{b}\\rbrace$について確認します． $\\boldsymbol{a}$は遷移行列であり，各要素$a_{ij}$が次の式を満たすものとします． 状態が$i$から$j$へ遷移するときの確率です． $$ a_{ij} = p(z_{t+1} = j | z_{t} = i) $$\nこれを集めて$N\\times N$の行列にすると遷移行列となります． 行ごとに見ると，$i$番目の行が状態$i$からの遷移確率を表したベクトルになっていることがわかります． $$ \\boldsymbol{a} = \\begin{bmatrix} a_{11} \u0026amp;\\cdots \u0026amp;a_{1N} \\\\ \\vdots \u0026amp;\\ddots \u0026amp;\\vdots \\\\ a_{N1} \u0026amp;\\cdots \u0026amp;a_{NN} \\end{bmatrix} $$\n$\\boldsymbol{b}$はある状態における音声特徴量の出力確率ベクトルです．各要素$b_i(\\boldsymbol{o}_t)$が次の式を満たします．出力確率ベクトルの各要素確率分布は多変量正規分布であると仮定します．ここで，$\\boldsymbol{\\mu}_i$は多変量正規分布の平均で，$\\boldsymbol{s}_i$は共分散行列の逆数である精度パラメータです． $$ \\begin{align} b_i(\\boldsymbol{o}_t) \u0026amp;= p(\\boldsymbol{o}_t | z_t = i) \\\\ \u0026amp;= \\mathcal{N}(\\boldsymbol{o}_t | z_t = i, \\boldsymbol{\\mu}_i, \\boldsymbol{s}_i^{-1}) \\end{align} $$\n各要素を集めると，$1\\times N$の出力確率ベクトルとなります． $$ \\boldsymbol{b} = (b_1(\\boldsymbol{o}_t), b_2(\\boldsymbol{o}_t), \\cdots, b_N(\\boldsymbol{o}_t)) $$\nパラメータについては終わりなので，隠れマルコフモデルのモデリングを行っていきます． 上で示したグラフィカルモデルでもいいのですが，再帰的な部分を含んでいるため，この部分を展開したようなグラフを描いて，モデリングしやすくします．このとき，パラメータも確率変数として扱って，グラフィカルモデル上に記述してしまいます．\nでは，このグラフィカルモデルから同時分布$p(\\boldsymbol{O}, \\boldsymbol{Z}, \\boldsymbol{\\Lambda})$を求めます．式(4)のようになります． $$ \\begin{align} p(\\boldsymbol{O}, \\boldsymbol{Z}, \\boldsymbol{\\Lambda}) \u0026amp; = p(\\boldsymbol{\\Lambda})p(\\boldsymbol{O} | \\boldsymbol{Z}, \\boldsymbol{\\Lambda})\\prod _{t=1}^{T-1} p(z _{t+1} | z_t, \\boldsymbol{\\Lambda}) \\nonumber \\\\ \u0026amp; = p(\\boldsymbol{a})p(\\boldsymbol{b})p(\\boldsymbol{O} | \\boldsymbol{Z}, \\boldsymbol{b})\\prod _{t=1}^{T-1}p(z _{t+1} | z_t, \\boldsymbol{a}) \\end{align} $$\n式(4)の右辺に含まれる因数についてそれぞれ考えていきます． $p(\\boldsymbol{O} | \\boldsymbol{Z}, \\boldsymbol{b})$について，観測データはi.i.dに従っていることを仮定していることから，式(5)が成り立ちます． $$ \\begin{equation} p(\\boldsymbol{O} | \\boldsymbol{Z}, \\boldsymbol{b}) = \\prod _{t = 1}^T p(\\boldsymbol{o}_t | z_t, \\boldsymbol{b}) \\end{equation} $$ ここで，$p(\\boldsymbol{o}_t | z_t, \\boldsymbol{b})$について考えます． よく考えると，この確率は出力確率そのものを表していることに気が付きます．式(1)，パラメータの確率分布の仮定を用いると，この確率分布は式(6)で書けます．この式では$t$と$i$が一致したときの出力確率を返すようにしています． $$ \\begin{align} p(\\boldsymbol{o}_t | z_t, \\boldsymbol{b}) \u0026amp;= \\prod _{i=1}^N b_i(\\boldsymbol{o}_t)^{z_t^i} \\nonumber \\\\ \u0026amp; = \\prod _{i=1}^N \\mathcal{N}(\\boldsymbol{o}_t | \\boldsymbol{\\mu}_i, \\boldsymbol{s}_i^{-1})^{z_t^i} \\end{align} $$\n式(5)と式(6)をまとめると，式(7)が得られます． $$ \\begin{equation} p(\\boldsymbol{O} | \\boldsymbol{Z}, \\boldsymbol{b}) = \\prod _{t=1}^T \\prod _{i=1}^N \\mathcal{N}(\\boldsymbol{o}_t | \\boldsymbol{\\mu}_i, \\boldsymbol{s}_i^{-1})^{z_t^i} \\end{equation} $$\n$p(z _{t+1} | z_t, \\boldsymbol{a})$について考えます． $t$と$i$が一致したときに，$z _{t+1}$に遷移する遷移確率を知りたいので， カテゴリ分布を用いることで確率分布を記述することができます．この確率分布は式(8)で書けます．\n$$ \\begin{align} p(z _{t+1} | z_t, \\boldsymbol{a}) \u0026amp;= \\prod _{i=1}^N \\text{Cat}(z _{t+1}, \\boldsymbol{a} _{i, :})^{z_t^i} \\nonumber \\\\ \u0026amp;= \\prod _{i=1}^N \\prod _{j=1}^N a _{ij}^{z _{t+1}^i \\cdot z_t^i} \\end{align} $$\n各因数の確率分布が分かったので，同時分布である式(4)に式(7)と式(8)を代入します． $$ \\begin{align} p(\\boldsymbol{O}, \\boldsymbol{Z}, \\boldsymbol{\\Lambda}) \u0026amp;= p(\\boldsymbol{a})p(\\boldsymbol{b})p(\\boldsymbol{O} | \\boldsymbol{Z}, \\boldsymbol{b})\\prod _{t = 1}^{T-1} p(z _{t+1} | z_t, \\boldsymbol{a}) \\nonumber \\\\ \u0026amp;= p(\\boldsymbol{a})p(\\boldsymbol{b})\\prod _{t=1}^T \\prod _{i=1}^N \\mathcal{N}(\\boldsymbol{o}_t | \\boldsymbol{\\mu}_i, \\boldsymbol{s}_i^{-1})^{z_t^i} \\prod _{t=1}^{T-1} \\prod _{i=1}^N \\prod _{j=1}^N a _{ij}^{z _{t+1}^i \\cdot z_t^i} \\end{align} $$\nよって，$\\boldsymbol{\\Lambda}$から出力される$\\boldsymbol{O}$，$\\boldsymbol{Z}$の確率分布は式(10)で表せます． $$ \\begin{equation} p(\\boldsymbol{O}, \\boldsymbol{Z} | \\boldsymbol{\\Lambda}) = \\prod _{t=1}^T \\prod _{i=1}^N \\mathcal{N}(\\boldsymbol{o}_t | \\boldsymbol{\\mu}_i, \\boldsymbol{s}_i^{-1})^{z_t^i} \\prod _{t=1}^{T-1} \\prod _{i=1}^N \\prod _{j=1}^N a _{ij}^{z _{t+1}^i \\cdot z_t^i} \\end{equation} $$\n計算を簡単にするため，対数尤度に変換します．これで，隠れマルコフモデルのモデリングができました． $$ \\begin{equation} \\ln p(\\boldsymbol{O}, \\boldsymbol{Z} | \\lambda) = \\sum _{t=1}^T \\sum _{i=1}^N \\ln\\mathcal{N}(\\boldsymbol{o}_t | \\boldsymbol{\\mu}_i, \\boldsymbol{s}_i^{-1})^{z_t^i} + \\sum _{t=1}^{T-1}\\sum _{i=1}^N \\sum _{j=1}^N z_t^i z _{t+1}^i \\ln a _{ij} \\end{equation} $$\n予測分布 事後分布の近似後は予測分布を求めることで，音声パラメータ$\\boldsymbol{Wx}$の生成を行えるようにします．ベイズ基準による音声合成は次式で与えられます．ここで，$\\boldsymbol{Wx}$は予測した音声特徴量，$\\boldsymbol{O}$は学習する音声特徴量，$s$は予測時(音声合成時)のラベル列，$S$は学習時のラベル列を表します．（下の式変形は音声特徴量$\\boldsymbol{Wx}$，$\\boldsymbol{O}$はラベル列$s$，$S$によって決まるからできるのだろうか？） $$ \\begin{align} \\boldsymbol{x}^{(Bayes)} \u0026amp;= \\argmax _{\\boldsymbol{x}} p(\\boldsymbol{Wx} | s, \\boldsymbol{O}, S) \\nonumber\\\\ \u0026amp;= \\argmax _{\\boldsymbol{x}}p(\\boldsymbol{Wx}, \\boldsymbol{O} | s, S) \\end{align} $$\n式(12)の確率分布$p(\\boldsymbol{Wx}, \\boldsymbol{O} | s, S)$は，ラベル列すなわち合成したいテキストから音声を生成する分布になっています．この分布は，学習データと合成したデータの二つを組み合わせた分布になっているので，これを学習データの分布と合成の分布の二つに分けたいです．そこで，確率の加法定理を利用して，学習の状態系列$\\boldsymbol{Z}$，合成の状態系列$\\boldsymbol{q}$，隠れマルコフモデルのパラメータ$\\boldsymbol{\\Lambda}$を導入します． $$ \\begin{equation} p(\\boldsymbol{Wx}, \\boldsymbol{O} | s, S) =\\sum _{\\boldsymbol{q}} \\sum _{\\boldsymbol{Z}} \\int p(\\boldsymbol{Wx}, \\boldsymbol{O}, \\boldsymbol{q}, \\boldsymbol{Z}, \\boldsymbol{\\Lambda} | s, S) d\\boldsymbol{\\Lambda} \\end{equation} $$\n被積分関数となっている確率分布$p(\\boldsymbol{Wx}, \\boldsymbol{O}, \\boldsymbol{q}, \\boldsymbol{Z}, \\boldsymbol{\\Lambda} | s, S)$について考えましょう．条件付き確率であるので，次の式が成り立ちます． $$ \\begin{equation} p(\\boldsymbol{Wx}, \\boldsymbol{O}, \\boldsymbol{q}, \\boldsymbol{Z}, \\boldsymbol{\\Lambda} | s, S) = \\frac{p(\\boldsymbol{Wx}, \\boldsymbol{O}, \\boldsymbol{q}, \\boldsymbol{Z}, \\boldsymbol{\\Lambda}, s, S)}{p(s, S)} \\end{equation} $$ 同時分布をグラフィカルモデルによってモデリングします． 次のようなグラフィカルモデルになると思います．学習と合成がそれぞれ独立な操作になっているので，確率変数自体も独立になっており，隠れマルコフモデルのパラメータ$\\boldsymbol{\\Lambda}$によってつながっているような状態になります．\nこのグラフィカルモデルから，同時分布を確率分布の積に変形します． $$ \\begin{align} p(\\boldsymbol{Wx}, \\boldsymbol{O}, \\boldsymbol{q}, \\boldsymbol{Z}, \\boldsymbol{\\Lambda}, s, S) \u0026amp;= p(\\boldsymbol{Wx} | \\boldsymbol{q})p(\\boldsymbol{q} | s, \\boldsymbol{\\Lambda})p(\\boldsymbol{O} | \\boldsymbol{Z})p(\\boldsymbol{Z} | S, \\boldsymbol{\\Lambda})p(\\boldsymbol{\\Lambda})p(s)p(S) \\nonumber\\\\ \u0026amp;= p(\\boldsymbol{Wx}, \\boldsymbol{q} | s, \\boldsymbol{\\Lambda})p(\\boldsymbol{O}, \\boldsymbol{Z} | S, \\boldsymbol{\\Lambda})p(\\boldsymbol{\\Lambda})p(s)p(S) \\end{align} $$\nよって，求めたかった被積分関数である確率分布$p(\\boldsymbol{Wx}, \\boldsymbol{O}, \\boldsymbol{q}, \\boldsymbol{Z}, \\boldsymbol{\\Lambda} | s, S)$は，式(16)と求まります． $$ \\begin{equation} p(\\boldsymbol{Wx}, \\boldsymbol{O}, \\boldsymbol{q}, \\boldsymbol{Z}, \\boldsymbol{\\Lambda} | s, S) = p(\\boldsymbol{Wx}, \\boldsymbol{q} | s, \\boldsymbol{\\Lambda})p(\\boldsymbol{O}, \\boldsymbol{Z} | S, \\boldsymbol{\\Lambda})p(\\boldsymbol{\\Lambda}) \\end{equation} $$\n式(13)に式(16)を代入します．これにより，合成の尤度と学習の尤度に分けることができました．この式が今回求めたい予測分布となります． $$ \\begin{align} p(\\boldsymbol{Wx}, \\boldsymbol{O} | s, S) \u0026amp;=\\sum _{\\boldsymbol{q}} \\sum _{\\boldsymbol{Z}} \\int p(\\boldsymbol{Wx}, \\boldsymbol{O}, \\boldsymbol{q}, \\boldsymbol{Z}, \\boldsymbol{\\Lambda} | s, S) d\\boldsymbol{\\Lambda} \\nonumber \\\\ \u0026amp;= \\sum _{\\boldsymbol{q}} \\sum _{\\boldsymbol{Z}} \\int p(\\boldsymbol{Wx}, \\boldsymbol{q} | s, \\boldsymbol{\\Lambda})p(\\boldsymbol{O}, \\boldsymbol{Z} | S, \\boldsymbol{\\Lambda})p(\\boldsymbol{\\Lambda}) d\\boldsymbol{\\Lambda} \\end{align} $$\n変分推論(変分下限の導出) 変分推論を行って，事後分布の近似を求めます． 確率分布$p(\\boldsymbol{Wx}, \\boldsymbol{O} | s, S)$に対して対数を取って対数尤度に変形して計算を行います．ここで，任意の分布$Q(\\boldsymbol{q}, \\boldsymbol{Z}, \\boldsymbol{\\Lambda})$を導入します．目標は，複雑な状態系列と隠れマルコフモデルのパラメータによる分布を簡単な分布$Q$で近似することです．途中の式変形ではイェンセンの不等式を用いました． $$ \\begin{align} \\log p(\\boldsymbol{Wx}, \\boldsymbol{O} | s, S) \u0026amp;= \\log \\sum _{\\boldsymbol{q}}\\sum _{\\boldsymbol{Z}}\\int p(\\boldsymbol{Wx}, \\boldsymbol{O}, \\boldsymbol{q}, \\boldsymbol{Z}, \\boldsymbol{\\Lambda} | s, S) d\\boldsymbol{\\Lambda} \\nonumber \\\\ \u0026amp;= \\log \\sum _{\\boldsymbol{q}}\\sum _{\\boldsymbol{Z}}\\int Q(\\boldsymbol{q}, \\boldsymbol{Z}, \\boldsymbol{\\Lambda})\\frac{p(\\boldsymbol{Wx}, \\boldsymbol{O}, \\boldsymbol{q}, \\boldsymbol{Z}, \\boldsymbol{\\Lambda} | s, S)}{Q(\\boldsymbol{q}, \\boldsymbol{Z}, \\boldsymbol{\\Lambda})} d\\boldsymbol{\\Lambda} \\nonumber\\\\ \u0026amp;\\geq \\sum _{\\boldsymbol{q}}\\sum _{\\boldsymbol{Z}}\\int Q(\\boldsymbol{q}, \\boldsymbol{Z}, \\boldsymbol{\\Lambda})\\log \\frac{p(\\boldsymbol{Wx}, \\boldsymbol{O}, \\boldsymbol{q}, \\boldsymbol{Z}, \\boldsymbol{\\Lambda} | s, S)}{Q(\\boldsymbol{q}, \\boldsymbol{Z}, \\boldsymbol{\\Lambda})} d\\boldsymbol{\\Lambda} \\nonumber \\\\ \u0026amp;=\\left\u0026lt; \\log \\frac{p(\\boldsymbol{Wx}, \\boldsymbol{O}, \\boldsymbol{q}, \\boldsymbol{Z}, \\boldsymbol{\\Lambda} | s, S)}{Q(\\boldsymbol{q}, \\boldsymbol{Z}, \\boldsymbol{\\Lambda})}\\right\u0026gt; _{Q(\\boldsymbol{q}, \\boldsymbol{Z}, \\boldsymbol{\\Lambda})} = \\mathcal{F} \\end{align} $$\n$\\mathcal{F}$という記号がでてきました．これは$Q(\\boldsymbol{q}, \\boldsymbol{Z}, \\boldsymbol{\\Lambda})$に対する変分下限（vasriational lower bound: VLB)もしくはELBO(evidence lower bound)と呼ばれるものです．周辺尤度$p(\\boldsymbol{Wx}, \\boldsymbol{O} | s, S)$の対数値の下限となっているためです．この量が重要になる理由は計算を進めるとわかります．計算を進めましょう．変分下限$\\mathcal{F}$を変形します． $$ \\begin{align} \\mathcal{F} \u0026amp;= \\left\u0026lt; \\log \\frac{p(\\boldsymbol{Wx}, \\boldsymbol{O}, \\boldsymbol{q}, \\boldsymbol{Z}, \\boldsymbol{\\Lambda} | s, S)}{Q(\\boldsymbol{q}, \\boldsymbol{Z}, \\boldsymbol{\\Lambda})}\\right\u0026gt; _{Q(\\boldsymbol{q}, \\boldsymbol{Z}, \\boldsymbol{\\Lambda})} \\nonumber \\\\ \u0026amp;= \\left\u0026lt;\\log\\frac{p(\\boldsymbol{q}, \\boldsymbol{Z}, \\boldsymbol{\\Lambda} | \\boldsymbol{Wx}, \\boldsymbol{O}, s, S)p(\\boldsymbol{Wx}, \\boldsymbol{O} | s, S)}{Q(\\boldsymbol{q}, \\boldsymbol{Z}, \\boldsymbol{\\Lambda})}\\right\u0026gt; _{Q(\\boldsymbol{q}, \\boldsymbol{Z}, \\boldsymbol{\\Lambda})} \\nonumber \\\\ \u0026amp;= \\sum _{\\boldsymbol{q}}\\sum _{\\boldsymbol{Z}}\\int Q(\\boldsymbol{q}, \\boldsymbol{Z}, \\boldsymbol{\\Lambda})\\log \\frac{p(\\boldsymbol{q}, \\boldsymbol{Z}, \\boldsymbol{\\Lambda} | \\boldsymbol{Wx}, \\boldsymbol{O}, s, S)p(\\boldsymbol{Wx}, \\boldsymbol{O} | s, S)}{Q(\\boldsymbol{q}, \\boldsymbol{Z}, \\boldsymbol{\\Lambda})} d\\boldsymbol{\\Lambda} \\nonumber \\\\ \u0026amp;= \\sum _{\\boldsymbol{q}}\\sum _{\\boldsymbol{Z}}\\int Q(\\boldsymbol{q}, \\boldsymbol{Z}, \\boldsymbol{\\Lambda})\\left\\lbrace \\log \\frac{p(\\boldsymbol{q}, \\boldsymbol{Z}, \\boldsymbol{\\Lambda} | \\boldsymbol{Wx}, \\boldsymbol{O}, s, S)}{Q(\\boldsymbol{q}, \\boldsymbol{Z}, \\boldsymbol{\\Lambda})} + \\log p(\\boldsymbol{Wx}, \\boldsymbol{O} | s, S)\\right\\rbrace \\nonumber \\\\ \u0026amp;= \\log p(\\boldsymbol{Wx}, \\boldsymbol{O} | s, S) - \\sum _{\\boldsymbol{q}}\\sum _{\\boldsymbol{Z}}\\int Q(\\boldsymbol{q}, \\boldsymbol{Z}, \\boldsymbol{\\Lambda})\\log \\frac{Q(\\boldsymbol{q}, \\boldsymbol{Z}, \\boldsymbol{\\Lambda})}{p(\\boldsymbol{q}, \\boldsymbol{Z}, \\boldsymbol{\\Lambda} | \\boldsymbol{Wx}, \\boldsymbol{O}, s, S)} d\\boldsymbol{\\Lambda} \\nonumber \\\\ \u0026amp;= \\log p(\\boldsymbol{Wx}, \\boldsymbol{O} | s, S) - \\text{KL}\\left[Q(\\boldsymbol{q}, \\boldsymbol{Z}, \\boldsymbol{\\Lambda}) || p(\\boldsymbol{q}, \\boldsymbol{Z}, \\boldsymbol{\\Lambda} | \\boldsymbol{Wx}, \\boldsymbol{O}, s, S) \\right] \\end{align} $$\n式(19)から，変分下限$\\mathcal{F}$は対数尤度$\\log p(\\boldsymbol{Wx}, \\boldsymbol{O} | s, S)$と仮定分布$Q(\\boldsymbol{q}, \\boldsymbol{Z}, \\boldsymbol{\\Lambda})$，状態系列，隠れマルコフモデルのパラメータの分布$p(\\boldsymbol{q}, \\boldsymbol{Z}, \\boldsymbol{\\Lambda} | \\boldsymbol{Wx}, \\boldsymbol{O}, s, S)$のKLダイバージェンスの差となっていることがわかります．操作するのは仮定分布$Q(\\boldsymbol{q}, \\boldsymbol{Z}, \\boldsymbol{\\Lambda})$なので，対数尤度は一定値として考えると，「KLダイバージェンスの最小化と変分下限の最大化は同値」であることがこの式から導けるのです．すなわち，変分下限の最大化を計算して，仮定分布$Q(\\boldsymbol{q}, \\boldsymbol{Z}, \\boldsymbol{\\Lambda})$を近似したい分布$p(\\boldsymbol{q}, \\boldsymbol{Z}, \\boldsymbol{\\Lambda} | \\boldsymbol{Wx}, \\boldsymbol{O}, s, S)$に近似しようと考えるのです．(KLダイバージェンスで直接計算最小化しない理由は，式中に近似したい未知の分布$p(\\boldsymbol{q}, \\boldsymbol{Z}, \\boldsymbol{\\Lambda} | \\boldsymbol{Wx}, \\boldsymbol{O}, s, S)$が存在しているため計算できないからです．)\nKLダイバージェンス$\\text{KL}\\left[Q(\\boldsymbol{q}, \\boldsymbol{Z}, \\boldsymbol{\\Lambda}) || p(\\boldsymbol{q}, \\boldsymbol{Z}, \\boldsymbol{\\Lambda} | \\boldsymbol{Wx}, \\boldsymbol{O}, s, S)\\right]$を具体的な確率分布に変形します．ここで，仮定分布$Q(\\boldsymbol{q}, \\boldsymbol{Z}, \\boldsymbol{\\Lambda})$を複雑にしないため，確率変数$\\boldsymbol{q}, \\boldsymbol{Z}, \\boldsymbol{\\Lambda}$が互いに独立であることを仮定します．このような仮定を平均場近似といいます． $$ \\begin{equation} Q(\\boldsymbol{q}, \\boldsymbol{Z}, \\boldsymbol{\\Lambda}) = Q(\\boldsymbol{q})Q(\\boldsymbol{Z})Q(\\boldsymbol{\\Lambda}) \\end{equation} $$\nKLダイバージェンスを変形していきましょう． $$ \\begin{align} \u0026amp;\\text{KL}\\left[Q(\\boldsymbol{q}, \\boldsymbol{Z}, \\boldsymbol{\\Lambda}) || p(\\boldsymbol{q}, \\boldsymbol{Z}, \\boldsymbol{\\Lambda} | \\boldsymbol{Wx}, \\boldsymbol{O}, s, S)\\right] \\nonumber \\\\ \u0026amp;= -\\sum _{\\boldsymbol{q}}\\sum _{\\boldsymbol{Z}}\\int Q(\\boldsymbol{q}, \\boldsymbol{Z}, \\boldsymbol{\\Lambda})\\log \\frac{p(\\boldsymbol{q}, \\boldsymbol{Z}, \\boldsymbol{\\Lambda} | \\boldsymbol{Wx}, \\boldsymbol{O}, s, S)}{Q(\\boldsymbol{q}, \\boldsymbol{Z}, \\boldsymbol{\\Lambda})} d\\boldsymbol{\\Lambda}\\nonumber \\\\ \u0026amp;= -\\sum _{\\boldsymbol{q}}\\sum _{\\boldsymbol{Z}}\\int Q(\\boldsymbol{q}, \\boldsymbol{Z}, \\boldsymbol{\\Lambda}) \\log p(\\boldsymbol{q}, \\boldsymbol{Z}, \\boldsymbol{\\Lambda} | \\boldsymbol{Wx}, \\boldsymbol{O}, s, S) d\\boldsymbol{\\Lambda} \\nonumber\\\\ \u0026amp;\\qquad + \\sum _{\\boldsymbol{q}}\\sum _{\\boldsymbol{Z}}\\int Q(\\boldsymbol{q}, \\boldsymbol{Z}, \\boldsymbol{\\Lambda})\\log Q(\\boldsymbol{q}, \\boldsymbol{Z}, \\boldsymbol{\\Lambda}) d\\boldsymbol{\\Lambda} \\nonumber \\\\ \u0026amp;= -\\sum _{\\boldsymbol{q}}\\sum _{\\boldsymbol{Z}}\\int Q(\\boldsymbol{q})Q(\\boldsymbol{Z})Q(\\boldsymbol{\\Lambda}) \\log p(\\boldsymbol{q}, \\boldsymbol{Z}, \\boldsymbol{\\Lambda} | \\boldsymbol{Wx}, \\boldsymbol{O}, s, S) d\\boldsymbol{\\Lambda} \\nonumber\\\\ \u0026amp;\\qquad + \\sum _{\\boldsymbol{q}}\\sum _{\\boldsymbol{Z}}\\int Q(\\boldsymbol{q})Q(\\boldsymbol{Z})Q(\\boldsymbol{\\Lambda})\\log Q(\\boldsymbol{q})Q(\\boldsymbol{Z})Q(\\boldsymbol{\\Lambda}) d\\boldsymbol{\\Lambda} \\end{align} $$\n式(21)の二項が複雑なので，(i)と(ii)と分けて計算を進めていきます．\n(i) 第一項について 第一項の式について計算していきます． $$ \\begin{align} \u0026amp;-\\sum _{\\boldsymbol{q}}\\sum _{\\boldsymbol{Z}}\\int Q(\\boldsymbol{q})Q(\\boldsymbol{Z})Q(\\boldsymbol{\\Lambda})\\log p(\\boldsymbol{q}, \\boldsymbol{Z}, \\boldsymbol{\\Lambda} | \\boldsymbol{Wx}, \\boldsymbol{O}, s, S) \\nonumber \\\\ \u0026amp;= -\\sum _{\\boldsymbol{q}}\\sum _{\\boldsymbol{Z}}\\int Q(\\boldsymbol{q})Q(\\boldsymbol{Z})Q(\\boldsymbol{\\Lambda})\\log \\frac{p(\\boldsymbol{q}, \\boldsymbol{Z}, \\boldsymbol{\\Lambda}, \\boldsymbol{Wx}, \\boldsymbol{O} | s, S)}{p(\\boldsymbol{Wx}, \\boldsymbol{O} | s, S)} d\\boldsymbol{\\Lambda}\\nonumber \\\\ \u0026amp;= -\\sum _{\\boldsymbol{q}}\\sum _{\\boldsymbol{Z}}\\int Q(\\boldsymbol{q})Q(\\boldsymbol{Z})Q(\\boldsymbol{\\Lambda})\\left\\lbrace \\log p(\\boldsymbol{q}, \\boldsymbol{Z}, \\boldsymbol{\\Lambda}, \\boldsymbol{Wx}, \\boldsymbol{O} | s, S) - \\log p(\\boldsymbol{Wx}, \\boldsymbol{O} | s, S)\\right\\rbrace d\\boldsymbol{\\Lambda}\\nonumber \\\\ \u0026amp;= \\log p(\\boldsymbol{Wx}, \\boldsymbol{O} | s, S) - \\sum _{\\boldsymbol{q}}\\sum _{\\boldsymbol{Z}}\\int Q(\\boldsymbol{q})Q(\\boldsymbol{Z})Q(\\boldsymbol{\\Lambda})\\log p(\\boldsymbol{q}, \\boldsymbol{Z}, \\boldsymbol{\\Lambda}, \\boldsymbol{Wx}, \\boldsymbol{O} | s, S) d\\boldsymbol{\\Lambda} \\end{align} $$\n式(22)の被積分関数の因数に確率分布$p(\\boldsymbol{Wx}, \\boldsymbol{O}, \\boldsymbol{q}, \\boldsymbol{Z}, \\boldsymbol{\\Lambda} | s, S)$の対数尤度が出てきました．これは予測分布を考える際に求めており，式(16)であることが分かっています．式(22)に代入して，計算を進めます． $$ \\begin{align} \u0026amp;-\\sum _{\\boldsymbol{q}}\\sum _{\\boldsymbol{Z}}\\int Q(\\boldsymbol{q})Q(\\boldsymbol{Z})Q(\\boldsymbol{\\Lambda})\\log p(\\boldsymbol{q}, \\boldsymbol{Z}, \\boldsymbol{\\Lambda} | \\boldsymbol{Wx}, \\boldsymbol{O}, s, S) \\nonumber \\\\ \u0026amp;= \\log p(\\boldsymbol{Wx}, \\boldsymbol{O} | s, S) \\nonumber \\\\ \u0026amp;\\qquad - \\sum _{\\boldsymbol{q}}\\sum _{\\boldsymbol{Z}}\\int Q(\\boldsymbol{q})Q(\\boldsymbol{Z})Q(\\boldsymbol{\\Lambda})\\log p(\\boldsymbol{Wx}, \\boldsymbol{q} | s, \\boldsymbol{\\Lambda})p(\\boldsymbol{O}, \\boldsymbol{Z} | S, \\boldsymbol{\\Lambda})p(\\boldsymbol{\\Lambda}) d\\boldsymbol{\\Lambda} \\nonumber \\\\ \u0026amp;= \\log p(\\boldsymbol{Wx}, \\boldsymbol{O} | s, S) \\nonumber \\\\ \u0026amp;\\qquad - \\sum _{\\boldsymbol{q}}\\sum _{\\boldsymbol{Z}}\\int Q(\\boldsymbol{q})Q(\\boldsymbol{Z})Q(\\boldsymbol{\\Lambda}) \\left\\lbrace \\log p(\\boldsymbol{Wx}, \\boldsymbol{q} | s, \\boldsymbol{\\Lambda}) + \\log p(\\boldsymbol{O}, \\boldsymbol{Z} | S, \\boldsymbol{\\Lambda}) + \\log p(\\boldsymbol{\\Lambda}) \\right\\rbrace d\\boldsymbol{\\Lambda} \\nonumber \\\\ \u0026amp;= \\log p(\\boldsymbol{Wx}, \\boldsymbol{O} | s, S) \\nonumber \\\\ \u0026amp;\\qquad -\\sum _{\\boldsymbol{q}}\\sum _{\\boldsymbol{Z}}\\int Q(\\boldsymbol{q})Q(\\boldsymbol{Z})Q(\\boldsymbol{\\Lambda})\\log p(\\boldsymbol{Wx}, \\boldsymbol{q} | s, \\boldsymbol{\\Lambda}) d\\boldsymbol{\\Lambda}\\nonumber \\\\ \u0026amp;\\qquad -\\sum _{\\boldsymbol{q}}\\sum _{\\boldsymbol{Z}}\\int Q(\\boldsymbol{q})Q(\\boldsymbol{Z})Q(\\boldsymbol{\\Lambda})\\log p(\\boldsymbol{O}, \\boldsymbol{Z} | S, \\boldsymbol{\\Lambda}) d\\boldsymbol{\\Lambda} \\nonumber \\\\ \u0026amp;\\qquad -\\sum _{\\boldsymbol{q}}\\sum _{\\boldsymbol{Z}}\\int Q(\\boldsymbol{q})Q(\\boldsymbol{Z})Q(\\boldsymbol{\\Lambda})\\log p(\\boldsymbol{\\Lambda}) d\\boldsymbol{\\Lambda} \\nonumber \\\\ \u0026amp;= \\log p(\\boldsymbol{Wx}, \\boldsymbol{O} | s, S) \\nonumber \\\\ \u0026amp;\\qquad -\\sum _{\\boldsymbol{q}}\\int Q(\\boldsymbol{q})Q(\\boldsymbol{\\Lambda})\\log p(\\boldsymbol{Wx}, \\boldsymbol{q} | s, \\boldsymbol{\\Lambda}) d\\boldsymbol{\\Lambda} \\nonumber \\\\ \u0026amp;\\qquad -\\sum _{\\boldsymbol{Z}}\\int Q(\\boldsymbol{Z})Q(\\boldsymbol{\\Lambda})\\log p(\\boldsymbol{O}, \\boldsymbol{Z} | S, \\boldsymbol{\\Lambda}) d\\boldsymbol{\\Lambda} \\nonumber \\\\ \u0026amp;\\qquad -\\int Q(\\boldsymbol{\\Lambda})\\log p(\\boldsymbol{\\Lambda}) d\\boldsymbol{\\Lambda} \\nonumber \\\\ \u0026amp;= \\log p(\\boldsymbol{Wx}, \\boldsymbol{O}| s, S) - \\left\u0026lt;\\log p(\\boldsymbol{Wx}, \\boldsymbol{q} | s, \\boldsymbol{\\Lambda})\\right\u0026gt; _{Q(\\boldsymbol{q})Q(\\boldsymbol{\\Lambda})} - \\left\u0026lt; \\log p(\\boldsymbol{O}, \\boldsymbol{Z} | S, \\boldsymbol{\\Lambda}) \\right\u0026gt; _{Q(\\boldsymbol{Z})Q(\\boldsymbol{\\Lambda})} - \\left\u0026lt;\\log p(\\boldsymbol{\\Lambda})\\right\u0026gt; _{Q(\\boldsymbol{\\Lambda})} \\end{align} $$\n(ii) 第二項について 第二項の式について計算していきます． $$ \\begin{align} \u0026amp;\\sum _{\\boldsymbol{q}} \\sum _{\\boldsymbol{Z}}\\int Q(\\boldsymbol{q})Q(\\boldsymbol{Z})Q(\\boldsymbol{\\Lambda})\\log Q(\\boldsymbol{q})Q(\\boldsymbol{Z})Q(\\boldsymbol{\\Lambda}) d\\boldsymbol{\\Lambda} \\nonumber \\\\ \u0026amp;= \\sum _{\\boldsymbol{q}}\\sum _{\\boldsymbol{Z}}\\int Q(\\boldsymbol{q})Q(\\boldsymbol{Z})Q(\\boldsymbol{\\Lambda})\\left\\lbrace \\log Q(\\boldsymbol{q}) + \\log Q(\\boldsymbol{Z}) + \\log Q(\\boldsymbol{\\Lambda})\\right\\rbrace d\\boldsymbol{\\Lambda} \\nonumber \\\\ \u0026amp;= \\sum _{\\boldsymbol{q}}\\sum _{\\boldsymbol{Z}}\\int Q(\\boldsymbol{q})Q(\\boldsymbol{Z})Q(\\boldsymbol{\\Lambda})\\log Q(\\boldsymbol{q}) d\\boldsymbol{\\Lambda} \\nonumber \\\\ \u0026amp; +\\sum _{\\boldsymbol{q}}\\sum _{\\boldsymbol{Z}}\\int Q(\\boldsymbol{q})Q(\\boldsymbol{Z})Q(\\boldsymbol{\\Lambda})\\log Q(\\boldsymbol{Z}) d\\boldsymbol{\\Lambda}\\nonumber \\\\ \u0026amp; +\\sum _{\\boldsymbol{q}}\\sum _{\\boldsymbol{Z}}\\int Q(\\boldsymbol{q})Q(\\boldsymbol{Z})Q(\\boldsymbol{\\Lambda})\\log Q(\\boldsymbol{\\Lambda}) d\\boldsymbol{\\Lambda}\\nonumber \\\\ \u0026amp;= \\sum _{\\boldsymbol{q}} Q(\\boldsymbol{q})\\log Q(\\boldsymbol{q}) + \\sum _{\\boldsymbol{Z}} Q(\\boldsymbol{Z})\\log Q(\\boldsymbol{Z}) + \\int Q(\\boldsymbol{\\Lambda})\\log Q(\\boldsymbol{\\Lambda})d\\boldsymbol{\\Lambda} \\nonumber \\\\ \u0026amp;= -H\\left[Q(\\boldsymbol{q})\\right] -H\\left[Q(\\boldsymbol{Z})\\right] -H\\left[Q(\\boldsymbol{\\Lambda})\\right] \\end{align} $$\n式(24)で出てきた$H\\left[\\cdot \\right]$は，情報エントロピーを表します．情報エントロピーは情報の乱雑さを表した指標です．数式的には，負の対数を取った確率分布のもとの確率分布による期待値です． すなわち，確率分布が$Q(\\boldsymbol{q})$の場合，次の式が成り立ちます． $$ \\begin{equation} H\\left[Q(\\boldsymbol{q})\\right] = \\left\u0026lt;-\\log Q(\\boldsymbol{q})\\right\u0026gt; _{Q(\\boldsymbol{q})} \\end{equation} $$\n(i)，(ii)が求まったので，式(21)に代入して求めたかったKLダイバージェンスを求めましょう． $$ \\begin{align} \u0026amp;\\text{KL}\\left[Q(\\boldsymbol{q}, \\boldsymbol{Z}, \\boldsymbol{\\Lambda}) || p(\\boldsymbol{q}, \\boldsymbol{Z}, \\boldsymbol{\\Lambda} | \\boldsymbol{Wx}, \\boldsymbol{O}, s, S)\\right] \\nonumber \\\\ \u0026amp;= -\\sum _{\\boldsymbol{q}}\\sum _{\\boldsymbol{Z}}\\int Q(\\boldsymbol{q})Q(\\boldsymbol{Z})Q(\\boldsymbol{\\Lambda}) \\log p(\\boldsymbol{q}, \\boldsymbol{Z}, \\boldsymbol{\\Lambda} | \\boldsymbol{Wx}, \\boldsymbol{O}, s, S) d\\boldsymbol{\\Lambda} \\nonumber\\\\ \u0026amp;\\qquad + \\sum _{\\boldsymbol{q}}\\sum _{\\boldsymbol{Z}}\\int Q(\\boldsymbol{q})Q(\\boldsymbol{Z})Q(\\boldsymbol{\\Lambda})\\log Q(\\boldsymbol{q})Q(\\boldsymbol{Z})Q(\\boldsymbol{\\Lambda}) d\\boldsymbol{\\Lambda} \\nonumber \\\\ \u0026amp;= \\log p(\\boldsymbol{Wx}, \\boldsymbol{O} | s, S) - \\left\u0026lt;\\log p(\\boldsymbol{Wx}, \\boldsymbol{q},| s, \\boldsymbol{\\Lambda})\\right\u0026gt; _{Q(\\boldsymbol{q})Q(\\boldsymbol{\\Lambda})} - \\left\u0026lt;\\log p(\\boldsymbol{O}, \\boldsymbol{Z} | S, \\boldsymbol{\\Lambda})\\right\u0026gt; _{Q(\\boldsymbol{Z})Q(\\boldsymbol{\\Lambda})} - \\left\u0026lt;\\log p(\\boldsymbol{\\Lambda})\\right\u0026gt; _{Q(\\boldsymbol{\\Lambda})} \\nonumber \\\\ \u0026amp;\\qquad -H\\left[Q(\\boldsymbol{q})\\right] - H\\left[Q(\\boldsymbol{Z})\\right] - H\\left[Q(\\boldsymbol{\\Lambda})\\right] \\end{align} $$\nKLダイバージェンスを確率分布で表現できました．式(19)に代入して変分下限$\\mathcal{F}$を求めます． $$ \\begin{align} \\mathcal{F} \u0026amp;= \\log p(\\boldsymbol{Wx}, \\boldsymbol{O} | s, S) - \\text{KL}\\left[Q(\\boldsymbol{q}, \\boldsymbol{Z}, \\boldsymbol{\\Lambda}) || p(\\boldsymbol{q}, \\boldsymbol{Z}, \\boldsymbol{\\Lambda} | \\boldsymbol{Wx}, \\boldsymbol{O}, s, S) \\right] \\nonumber \\\\ \u0026amp;= \\log p(\\boldsymbol{Wx}, \\boldsymbol{O} | s, S) - \\log p(\\boldsymbol{Wx}, \\boldsymbol{O} | s, S) + \\left\u0026lt;\\log p(\\boldsymbol{Wx}, \\boldsymbol{q},| s, \\boldsymbol{\\Lambda})\\right\u0026gt; _{Q(\\boldsymbol{q})Q(\\boldsymbol{\\Lambda})} \\nonumber \\\\ \u0026amp;\\qquad + \\left\u0026lt;\\log p(\\boldsymbol{O}, \\boldsymbol{Z} | S, \\boldsymbol{\\Lambda})\\right\u0026gt; _{Q(\\boldsymbol{Z})Q(\\boldsymbol{\\Lambda})} + \\left\u0026lt;\\log p(\\boldsymbol{\\Lambda})\\right\u0026gt; _{Q(\\boldsymbol{\\Lambda})} +H\\left[Q(\\boldsymbol{q})\\right] + H\\left[Q(\\boldsymbol{Z})\\right] + H\\left[Q(\\boldsymbol{\\Lambda})\\right] \\nonumber \\\\ \u0026amp;= \\left\u0026lt;\\log p(\\boldsymbol{Wx}, \\boldsymbol{q},| s, \\boldsymbol{\\Lambda})\\right\u0026gt; _{Q(\\boldsymbol{q})Q(\\boldsymbol{\\Lambda})} + \\left\u0026lt;\\log p(\\boldsymbol{O}, \\boldsymbol{Z} | S, \\boldsymbol{\\Lambda})\\right\u0026gt; _{Q(\\boldsymbol{Z})Q(\\boldsymbol{\\Lambda})} + \\left\u0026lt;\\log p(\\boldsymbol{\\Lambda})\\right\u0026gt; _{Q(\\boldsymbol{\\Lambda})} \\nonumber \\\\ \u0026amp;\\qquad +H\\left[Q(\\boldsymbol{q})\\right] + H\\left[Q(\\boldsymbol{Z})\\right] + H\\left[Q(\\boldsymbol{\\Lambda})\\right] \\end{align} $$\nこれで，未知分布なしで変分下限$\\mathcal{F}$が求まったので，この値を最大化することで，仮定分布$Q(\\boldsymbol{q}, \\boldsymbol{Z},\\boldsymbol{\\Lambda})$を求めることができます．結果として，最適な予測分布$p(\\boldsymbol{Wx}, \\boldsymbol{O} | s, S)$を求めることができます．変分下限の最大化を求める際に，勾配降下法などの最適化手法を用いる場合は，負の値を取って$\\mathcal{L} = -\\mathcal{F}$として，$\\mathcal{L}$の最小化を考えれば良いです．\n終わりに 音声合成は好きですが，このように理論を数式で追うのは今回初めて行いました． 理論はベイズ機械学習が軸にあるのでまずはそこを勉強して，次に応用として音声合成の手法の一つである隠れマルコフモデルによる手法をベイズ機械学習の知見を元に勉強していきました．理論はベイズ機械学習でもそうですがとても難しいと思います．しかし，ベイズ機械学習を使うことでで不確実なデータに対してモデリングして，それを生成するようなモデルを作れるというのはとても面白いと思います． 今回も複雑な理論の部分もありましたが，ベイズ機械学習の理論のもとで，音声を合成するという難しい問題を近似的に解くことができるということが理論で追うことができて，凄さと面白さを垣間見た感じがします．\n理論を少し理解できたので，今度は実装を行ってみたいと思います．（これで理論でよくわからなかったところも理解できるか？）\n補足 数式の変形の中で現れた，期待値とKLダイバージェンスについてと音声合成そのものに対して補足します．\n期待値 期待値は，確率変数の全ての値に対して，その値が起こりうる確率の重みをかけた加重平均です． 確率変数$\\boldsymbol{x}$をベクトルとしたとき，確率分布$p(\\boldsymbol{x})$に対してある関数$f(\\boldsymbol{x})$の期待値$\\left\u0026lt;f(\\boldsymbol{x})\\right\u0026gt; _{p(\\boldsymbol{x})}$は次の式で定義されます． $$ \\left\u0026lt;f(\\boldsymbol{x})\\right\u0026gt; _{p(\\boldsymbol{x})} = \\int f(\\boldsymbol{x})p(\\boldsymbol{x}) d\\boldsymbol{x} $$\n期待値の記号は他にも$\\mathbb{E} _{p(\\boldsymbol{x})}\\left[f(\\boldsymbol{x})\\right]$のような期待値の頭文字(Expected Value)を取ったものもあります．\nKLダイバージェンス KLダイバージェンス(Kullback-Leibler divergence: KL divergence)は 真の確率分布$p(\\boldsymbol{x})$とそれ以外の任意の確率分布$q(\\boldsymbol{x})$ の差異を表した指標です．任意の確率分布$q(\\boldsymbol{x})$として，KLダイバージェンスの最小化問題を解くことにより，真の確率分布$p(\\boldsymbol{x})$に近似した確率分布を求めることができるという重要な役割を持ちます．KLダイバージェンスは次の式で定義されます． $$ \\text{KL}\\left[q(\\boldsymbol{x})||p(\\boldsymbol{x})\\right] = \\int q(\\boldsymbol{x})\\log \\frac{q(\\boldsymbol{x})}{p(\\boldsymbol{x})} d\\boldsymbol{x} $$\n右辺について計算を進めます． $$ \\begin{align} \\int q(\\boldsymbol{x})\\log \\frac{q(\\boldsymbol{x})}{p(\\boldsymbol{x})} d\\boldsymbol{x} \u0026amp;= \\int - q(\\boldsymbol{x})\\log p(\\boldsymbol{x}) d\\boldsymbol{x}- \\int - q(\\boldsymbol{x})\\log q(\\boldsymbol{x}) d\\boldsymbol{x} \\nonumber \\\\ \u0026amp;= \\left\u0026lt;-\\log p(\\boldsymbol{x})\\right\u0026gt; _{q(\\boldsymbol{x})} - \\left\u0026lt;-\\log q(\\boldsymbol{x})\\right\u0026gt; _{q(\\boldsymbol{x})} \\nonumber \\\\ \u0026amp;= H\\left[q(\\boldsymbol{x}), p(\\boldsymbol{x})\\right] - H\\left[p(\\boldsymbol{x})\\right] \\geq 0 \\end{align} $$\n$H\\left[q(\\boldsymbol{x}), p(\\boldsymbol{x})\\right]$は交差エントロピーと呼ばれ，2つの確率分布の間に定義される尺度です．確率分布$p$, $q$間でずれが起きると予測はしにくくなり，交差エントロピーは大きくなります．また，最小値は$p=q$の時で$H\\left[p(\\boldsymbol{x})\\right]$と同値となることから，次のことが成り立ちます．KLダイバージェンスの最後の式ではこの式を用いています． $$ H\\left[q(\\boldsymbol{x}), p(\\boldsymbol{x})\\right] \\geq H\\left[p(\\boldsymbol{x})\\right] $$\n合成音声とキャラクター 合成音声を調べると，可愛いもしくはかっこいいキャラクターの立ち絵が音声に対してついていることがほとんどだと思います． これは，2004年にクリプトン・フューチャー・メディアから発売されたVOCALOID2用歌声ライブラリMEIKOのパッケージに声のモチーフとなるような女の子のイラストがつけられたことが始まりです．KAITO，CVシリーズから初音ミク，鏡音リン・レン，巡音ルカと声に対してキャラクターが考えられて，パッケージに描かれるようになりました．これはVOCALOIDだけでなく，VOICEROIDなどの他の音声合成ソフトウェアにも影響を与え，VOICEROIDでは結月ゆかり，紲星あかりが発売，VOICEROID+では琴葉茜・葵\u0026hellip;といった流れに変わり，合成音声にはモチーフとなるキャラクターを与えるということが実質標準化していきました． 合成音声（歌声）に対してモチーフとなるキャラクターを与えることによって，ユーザーは音声合成ソフトを使うというハードルが下がるなどの利点があります．また，キャラクターがつけられたことで，動画や音楽などのエンターテイメント分野でたくさんの人が使うようになり，ニコニコ動画などの動画サイトを中心に音声（歌声）合成の文化が広がっていたという面白い面もあります．\n参考文献  南角吉彦, 全炳河, 徳田恵一, 北村正, 益子貴史，「ベイズ的アプローチに基づくHMM音声合成」『信学技報』，電子情報通信学会 須山敦司著，杉山将監修，『ベイズ推論による機械学習入門』 山元竜一，高道慎之介著，『Pythonで学ぶ音声合成』 才野慶二郎（ヤマハ株式会社），「歌声の合成における応用技術」『日本音響学会誌 75巻7号』，日本音響学会(2019) みさいる，「【足立レイ】中の人のいない合成音声作った【UTAU音源配布】」，https://www.nicovideo.jp/watch/sm33386516 飴屋／菖蒲，「UTAU」，http://utau2008.xrea.jp/ YAMAHA，「VOCALOID」，https://www.vocaloid.com/ CeVIOプロジェクト，「CeVIO」，https://cevio.jp/ 名古屋工業大学，「Open JTalk」，https://open-jtalk.sp.nitech.ac.jp/ 名古屋工業大学，「Sinsy」，https://www.sinsy.jp/ 株式会社ai，「AITalk」，https://www.ai-j.jp/about/ Dreamtonics株式会社，「Synthesizer V」，https://dreamtonics.com/synthesizerv/ Google，「Tacotron: An end-to-end speech synthesis system by Google」，https://google.github.io/tacotron/index.html   [1] VOCALOID(初代)は，音素選択によって音声合成を行っていますが，VOCALOID5では，音素選択と統計モデルによる手法を組み合わせて音声合成を行っているようです [2] VOICEROID，A.I.VOICEのベースとなるエンジンのAITalkはバージョンアップによって，AITalk5でコーパスベース方式だけでなく，DNNによる音声合成も行えるようになっています．A.I.VOICEはAITalk5のエンジンを使っています．\n ","date":"2022-12-19T00:00:00+09:00","image":"https://hiro-1219.github.io/blog/p/%E9%9A%A0%E3%82%8C%E3%83%9E%E3%83%AB%E3%82%B3%E3%83%95%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AB%E3%82%88%E3%82%8B%E9%9F%B3%E5%A3%B0%E5%90%88%E6%88%90%E3%82%92%E7%90%86%E8%A7%A3%E3%81%99%E3%82%8B/img/HMM_model_hua8e4d20db419a61d3690be023716ce75_30988_120x120_fill_box_smart1_3.png","permalink":"https://hiro-1219.github.io/blog/p/%E9%9A%A0%E3%82%8C%E3%83%9E%E3%83%AB%E3%82%B3%E3%83%95%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AB%E3%82%88%E3%82%8B%E9%9F%B3%E5%A3%B0%E5%90%88%E6%88%90%E3%82%92%E7%90%86%E8%A7%A3%E3%81%99%E3%82%8B/","title":"隠れマルコフモデルによる音声合成を理解する"},{"content":"はじめに この記事は「長野高専 Advent Calendar 2022」16日目に合わせて書かれた記事です． おひさしぶりです．やじるしです．Advent Calendar 2022の季節がやってまいりました（この記事を出すのは後半戦）．今回は，機械学習を確率論からアプローチした「ベイズ機械学習」についてまとめたいと思います．自分が理解できるように噛み砕いて書くつもりです．\n機械学習とはなにか ベイズ機械学習を始める前に，そもそも機械学習とはなんなのか示さないといけません． トム・M・ミッチェルの学習の定義を引用します．\n コンピュータプログラムがタスクのクラス$T$と性能指標$P$に関し経験$E$から学習するとは，$T$内のタスクの$P$で測った性能が経験$E$により改善されることを言う．\n これを噛み砕くと，何かしらのタスクに対して，手元にあるデータセットから問題に対する性能が上がるように改善を繰り返すアルゴリズムのことを学習と定義しています．タスクに対してモデルを 作り，そのモデルにデータセットを渡して学習を行うことで，モデルは複雑なデータの規則や構造をよりよく表したものになるはずです．そのようなモデルを作り出すことができれば，私達はそのモデルを使用して様々な問題に対する予測や分類，データ生成を行うことができます．\n確率に関する諸定理 ベイズ機械学習を理解するにあたって，基本的な確率の操作を理解しておく必要があります． ここでは，確率分布，加法定理・乗法定理，独立性，ベイズの定理を説明します．\n確率分布 確率を導入するにあたり，はじめに確率分布を定義します． 確率分布はある値に対して，その値がどれだけの確率で現れるかを表したものです． 確率分布を導入することで，不確定性があるようなパラメータなども確率という整えられたフレームワーク内で操作することが可能になります．\n各要素が連続値であるような$M$次元ベクトル$\\boldsymbol{x} = (x_1, x_2, \\cdots x_M)^\\top \\in \\mathbb{R}^M$(確率変数)に対する関数$p(\\boldsymbol{x})$が2つの条件を満たすとき，これを確率密度関数といいます． $$ p(\\boldsymbol{x}) \\geq 0 \\\\ \\int p(\\boldsymbol{x}) d\\boldsymbol{x} = \\int \\cdots \\int p(x_1, \\cdots x_M) dx_1 \\cdots dx_M = 1 $$\n$\\boldsymbol{x}$の要素が離散値の場合，これを確率質量関数といいます．$\\boldsymbol{x}$の要素が離散値になったことで積分記号の極限が外れて総和記号になっただけで，確率密度関数と表していることは同じです． $$ p(\\boldsymbol{x}) \\geq 0 \\\\ \\sum_{\\boldsymbol{x}} p(\\boldsymbol{x}) = \\sum_{x_1} \\cdots \\sum_{x_M} p(x_1, \\cdots x_M) = 1 $$\nこれら，確率密度関数と確率質量関数を合わせて確率分布といいます． 以下の説明から，連続値についてのみ説明します．ただし，離散値の場合も同じものが成り立つので，必要になった場合は適時離散値に置き換えてください．\n加法定理，乗法定理 確率分布を操作するために必要な性質は2つしかありません．確率の加法定理(1)と乗法定理(2)です．確率分布の全操作は加法定理と乗法定理の組み合わせです．そのため，これさえ理解すれば確率分布を自由に操作できるはずです． $$ \\begin{equation} p(\\boldsymbol{x}) = \\int p(\\boldsymbol{x}, \\boldsymbol{y}) d\\boldsymbol{y} \\end{equation} $$ $$ \\begin{equation} p(\\boldsymbol{x}, \\boldsymbol{y}) = p(\\boldsymbol{x} | \\boldsymbol{y})p(\\boldsymbol{y}) = p(\\boldsymbol{y} | \\boldsymbol{x})p(\\boldsymbol{x}) \\end{equation} $$\n$p(\\boldsymbol{x}, \\boldsymbol{y})$は同時分布とよばれ，ある$\\boldsymbol{x}$と$\\boldsymbol{y}$が同時に起こる確率がどれだけかを表した確率分布です．加法定理，乗法定理ともに，同時分布を何かしらして違う分布に変えるという操作になっています． 加法定理は，確率変数$\\boldsymbol{y}$に関して，全ての取りうる値を加算していくことで，$\\boldsymbol{y}$を削除する操作です．周辺化とも呼ばれ，得られる確率分布$p(\\boldsymbol{x})$は周辺分布と呼ばれます．乗法定理では，条件付き確率の定義そのものです．条件付き確率$p(\\boldsymbol{x} | \\boldsymbol{y})$は条件となる確率変数$\\boldsymbol{y}$の元で，確率変数$\\boldsymbol{x}$がどれだけ現れるかを表した確率です．\n独立性 条件付き確率$p(\\boldsymbol{x} | \\boldsymbol{y})$が出てきましたが，確率変数$\\boldsymbol{x}$の値の出方は確率変数$\\boldsymbol{y}$に影響しないという場合は多くあります．例えば，サイコロが2つA, Bとあったとき，Aを投げて出た値を観測した後に，Bを投げた時，観測されたBの出目はAに影響するでしょうか．何かしらの細工がない限り影響しないと思います．このような状態であるとき，$p(\\boldsymbol{x} | \\boldsymbol{y}) = p(\\boldsymbol{x})$となり，乗法定理に代入すると，式(3)が得られます． $$ \\begin{equation} p(\\boldsymbol{x}, \\boldsymbol{y}) = p(\\boldsymbol{x})p(\\boldsymbol{y}) \\end{equation} $$ これは嬉しい性質です．それぞれの確率変数間で関係性が無い時，周辺分布（確率）はそれぞれの確率変数の確率分布の積で表せます．このような性質を独立性と呼びます．また，この性質は必要十分条件であり，確率変数$\\boldsymbol{x}$と$\\boldsymbol{y}$が互いに独立である時，式(3)を満たします．確率変数が$\\boldsymbol{x}$と$\\boldsymbol{y}$だけでなく，複数個あったときにもこの性質はもちろん成り立ちます．確率変数列$\\boldsymbol{X} = (\\boldsymbol{x}_1, \\cdots, \\boldsymbol{x}_N)^\\top$があった時，式(4)が成り立ちます． $$ \\begin{equation} p(\\boldsymbol{X}) = p(\\boldsymbol{x}_1, \\cdots \\boldsymbol{x}_N) = \\prod _{i=1}^{N} p(\\boldsymbol{x}_i) \\end{equation} $$\nまた，新たに$\\boldsymbol{x}$と$\\boldsymbol{y}$に対して新たに条件$\\boldsymbol{z}$がついた時は式(5)が成り立ちます．これを条件付き独立性と呼びます．この性質も複数個の確率変数があった時にも成り立ちます． $$ \\begin{equation} p(\\boldsymbol{x}, \\boldsymbol{y} | \\boldsymbol{z}) = p(\\boldsymbol{x} | \\boldsymbol{z})p(\\boldsymbol{y} | \\boldsymbol{z}) \\end{equation} $$\n複数個ある確率変数$\\boldsymbol{x}_1, \\cdots \\boldsymbol{x}_N$がある一つの確率分布に従うという仮定のもとで，独立性が保証されているとき，これは，独立同分布(independent and identically distribute: i.i.d)であるといいます． 学習データは独立同分布に従うことを仮定して議論を進めていきます． この仮定があることにより，学習データの同時分布が式(4)により求めることができます．（共分散などで各確率変数間の関係を考える必要がありません）\nベイズの定理 乗法定理(2)より，二つの確率変数$\\boldsymbol{x}$と$\\boldsymbol{y}$の間には，次のようなことが成り立つことがわかります． $$ p(\\boldsymbol{x}, \\boldsymbol{y}) = p(\\boldsymbol{x} | \\boldsymbol{y})p(\\boldsymbol{y}) = p(\\boldsymbol{y} | \\boldsymbol{x})p(\\boldsymbol{x}) $$\nここで，両辺を$p(\\boldsymbol{y})$で割ると式(6)が得られます． $$ \\begin{equation} p(\\boldsymbol{x} | \\boldsymbol{y}) = \\frac{p(\\boldsymbol{y} | \\boldsymbol{x})p(\\boldsymbol{x})}{p(\\boldsymbol{y})} \\end{equation} $$\nこの式が表すものこそが，ベイズの定理と呼ばれるものです． 確率の乗法定理を変形しただけのものがなぜこんなにも使われ，「ベイズ統計」，「ベイズ推論」，「ベイズ機械学習」など様々な分野で使われているのでしょうか．これは，ベイズの定理の式(6)の意味を考えるとわかります．原因となる確率変数を$\\boldsymbol{x}$とし，結果となる確率変数を$\\boldsymbol{y}$とします．右辺にある$p(\\boldsymbol{y} | \\boldsymbol{x})$は原因となる$\\boldsymbol{x}$によって結果$\\boldsymbol{y}$が決まるという「原因→結果」の順番になっています．左辺はどうなっているでしょうか，左辺は結果$\\boldsymbol{y}$から原因$\\boldsymbol{x}$が決まるという「結果→原因」の順番になっていることに気が付きます．ベイズの定理がすごいところは「原因→結果」の確率がわかると，「結果→原因」となる確率を調べることができる点です．確率の乗法定理をただ変形させた定理ですが，視点を変えると結果から原因を調べられてしまうという強力な定理になります．\n式中に含まれる確率分布について説明します．$p(\\boldsymbol{x})$について，結果$\\boldsymbol{y}$を観測する前の原因となりうる事前知識を確率分布として仮定できることから事前分布と呼ばれます．$p(\\boldsymbol{x} | \\boldsymbol{y})$は結果$\\boldsymbol{y}$を観測した後の原因$\\boldsymbol{x}$の確率分布となっていることから事後分布と呼ばれます．\nベイズ機械学習 ベイズ機械学習は，確率モデリングと確率推論を利用した機械学習のアプローチです．\n観測データ$\\boldsymbol{D}$が観測された状態を考えます． 私達は，観測データを構築したモデルに学習させることで，未知のデータに対しても 予測や分類，もしくは新たにデータを生成させるようなタスクを行うことを目標とします．\nモデルの構築 まずはじめに，学習する確率モデルを考えます．確率モデルを考えるにあたり，観測されたデータ$\\boldsymbol{X} = (x_1, x_2, \\cdots x_N)^T$がそのようにして得られたのかという生成過程を考えることになります．このように，観測データの生成過程を記述することで，人工的なデータのシミュレーションを行うようにしたモデルを生成モデルと呼びます．生成モデルを構築する際には，確率変数間の依存関係をグラフにより表現したグラフィカルモデルにより記述します．グラフィカルモデルはグラフを用いた表現になるので，視覚的にも分かりやすいです． ここでは，次のようなグラフィカルモデルを考えます．[1]\nパラメータ$\\theta$によって観測データ$\\boldsymbol{X}$が生成されたという構造になっています．これを用いて，同時分布$p(\\boldsymbol{X}, \\theta)$を考えます．同時分布$p(\\boldsymbol{X}, \\theta)$は式(7)で表せます． $$ \\begin{align} p(\\boldsymbol{X}, \\theta) \u0026amp;= p(\\boldsymbol{X} | \\theta)p(\\theta) \\nonumber\\\\ \u0026amp;= \\left\\lbrace\\prod _{n=1}^N p(x_n | \\theta)\\right\\rbrace p(\\theta) \\end{align} $$\nここで，$p(x_n | \\theta)$はパラメータ$\\theta$からデータ$x_n$がどのようにして発生しているかを表す尤度関数です．また，$p(\\theta)$は，モデルに含まれるパラメータについての確率分布であり，ベイズの定理でいう事前分布となります．これから，観測データ$\\boldsymbol{X}$によって，パラメータ$\\theta$の確率分布がどのようになるか，すなわち事後分布$p(\\theta | \\boldsymbol{X})$を考えることになります．\nパラメータの事後分布 パラメータの事後分布$p(\\theta | \\boldsymbol{X})$を考えます．ベイズの定理より， 式(8)が言えます． $$ \\begin{align} p(\\theta | \\boldsymbol{X}) \u0026amp;= \\frac{\\left\\lbrace\\prod _{n=1}^N p(x_n | \\theta)\\right\\rbrace p(\\theta)}{p(\\boldsymbol{X})} \\nonumber\\\\ \u0026amp;\\propto \\left\\lbrace\\prod _{n=1}^N p(x_n | \\theta)\\right\\rbrace p(\\theta) \\end{align} $$\nベイズ機械学習では，事後分布$p(\\theta | \\boldsymbol{X})$を求めることこそが学習となります．\nベイズの定理により現れた分母$p(\\boldsymbol{X})$は式(9)でかけます． $$ \\begin{equation} p(\\boldsymbol{X}) = \\int p(\\boldsymbol{X} | \\theta)p(\\theta) d\\theta \\end{equation} $$ これは，周辺尤度と呼ばれ，ベイズ機械学習により得られたパラメータの確率分布$p(\\theta)$のもとで，パラメータ$\\theta$により生成されたデータ$\\boldsymbol{X}$に関する生成確率$p(\\boldsymbol{X} | \\theta)$の期待値となっていることから，モデルの性能を調べることができるモデルエビデンスとなっています．\n学習 パラメータの事後分布$p(\\theta | \\boldsymbol{X})$を求め，どのような確率分布になっているかを調べることが学習となります．なので，実際にパラメータの事後分布$p(\\theta | \\boldsymbol{X})$を計算していき，事後分布がどのような分布なのかを求めていきます．しかし，事前分布が共役事前分布の場合や，簡単なモデルを構築した場合は解析的に求めることができますが，そうでないような複雑な事前分布を仮定している，もしくは複雑なモデルを構築している場合は解析的に解けません．そこで，事後分布を解析的にではなく，事後分布を近似する手法を取ります． 確率分布を近似する手法としてMCMC法と変分推論法が挙げられます．\n MCMC法 : マルコフ連鎖モンテカルロ法．サンプリングを応用した近似手法． 変分推論法 : 簡単な確率分布を仮定して，それと事後分布を比較することにより近似する手法．  予測分布 学習されたパラメータの事後分布から，未知のデータ$x_\\star$に対して知見を得たいです． なので，予測分布$p(x_\\star | \\boldsymbol{X})$を求めます．確率の加法定理などを使って，式を変形します． $$ p(x_\\star | \\boldsymbol{X}) = \\int p(x_\\star | \\theta)p(\\theta | \\boldsymbol{X}) d\\theta $$\nここで，被積分関数の因数2つに注目します． $p(x_\\star | \\theta)$はパラメータ$\\theta$から未知データ$x_\\star$予測するという識別モデルになっており，$p(\\theta | \\boldsymbol{X})$は先ほど学習した事後分布になっています． すなわち，予測分布$p(x_\\star | \\boldsymbol{X})$は，あらゆるパラメータに関しての加重平均を求めることで，予測の不確かさを表すことができるのです．\nベイズ機械学習の利点 ベイズ機械学習は確率モデリングと確率推論を利用した手法であることから最尤推定や誤差関数の最適化による機械学習に比べて，次のような利点を持ちます．\n 様々な問題に対して一貫性を持って解くことができる　様々な問題に対して，グラフィカルモデルにより生成過程を記述することで確率モデルにすることができます．例えば，次のような問題に対して適用することができます．  線型回帰 クラスタリング 次元削減 隠れマルコフモデル など   対象の不確実性を定量的に扱うことができる ベイズ機械学習により求めた予測分布は確率分布であるので，確率の世界で不確実性を定量的に扱えます． 自分が持っている知見を自然に取り入れることができる モデリングの時点で，事前分布に自分の持っている知見を取り入れることができます． 過学習が起こりにくい  確率的プログラミング言語 ベイズ機械学習を行うためには，確率分布の近似アルゴリズム（ベイズ機械学習では学習にあたるアルゴリズム）であるMCMC法や変分推論法を実装する必要があります．しかし，ここで集中して行いたい作業は確率モデルのモデリングです．そこで，確率的プログラミング言語(Probabilistic Programming Language: PPL)を使用して確率モデルのモデリングと推論を行います．確率的プログラミングは確率モデルを指定したときに，このモデルの推論を自動的に実行してくれるプログラミングパラダイムです．このパラダイムでは，推論を自動的に行ってくれるので，私達は確率モデルのモデリングにだけ集中できます．確率プログラミング言語とは言われていますが，そのためだけの言語として作られることは少なく，Python，C++などの高級言語の拡張として作られていることが多いです．\nPythonでは確率的プログラミング言語はパッケージとして提供されており，次のものが挙げられます．\n PyStan : PythonでStanを使うパッケージ PyMC3 : TheanoがバックエンドのPPL TensorFlow Probability : Google製深層学習フレームワークTensorFlowをバックエンドとしたPPL Pyro : Uberが開発したPyTorchバックエンドのPPL．変分推論が強いイメージ． NumPyro : PyroのJAX(Numpyに自動微分機能などをつけたGoogle製の線形代数ライブラリ)バックエンド版．MCMC法が速くなった． Pixyz : 東京大学 松尾研究室の鈴木雅大さんが中心となって開発した深層生成モデルを作成するためのPyTorchバックエンドのパッケージ．ELBOなどを求めてloss関数を実装するのは自分で行うが，その数式そのものをプログラムで実装できる，ニューラルネットワークを自然に取り込めることが特徴．  線形回帰 では，実際に確率的プログラミング言語を使用してベイズ機械学習を行います． 今回は線形回帰を行ってみたいと思います．確率的プログラミング言語はNumPyroを使ってみます．\n問題設定 問題設定を確認しましょう．\n $\\boldsymbol{X} = (x_1, x_2, \\cdots, x_N)^\\top$とそれに対する目標値$\\boldsymbol{Y} = (y_1, y_2, \\cdots, y_N)^\\top$に基づいて，新たな入力値$x$に関する目標変数$y$の予測を行う．\n 今回はテストデータセット$\\boldsymbol{D} = \\lbrace \\boldsymbol{X}, \\boldsymbol{Y}\\rbrace$は次の関係を満たすようなものとします．（本来はここは分かっていません） $$ y = \\sin{(2\\pi x)} + \\varepsilon \\\\ \\epsilon \\sim \\mathcal{N}(0, 0.2) $$\n実際にグラフにプロットしてみます．0から1の範囲で50点だけデータを取ると，グラフは以下の図のようになりました．理想的には$y = \\sin{(2\\pi x)}$になっていてほしいです．\nデータセットは次のソースコードで作成しました．乱数のシード値は固定しています．\n1 2 3  N = 100 X = jnp.linspace(0, 1, N) Y = jnp.sin(2 * jnp.pi * X) + np.random.normal(0, 0.2, (N, ))   モデルの構築 学習する確率モデルを構築しましょう． 観測された入力データ$\\boldsymbol{X}$，出力データ$\\boldsymbol{Y}$， 未知のパラメータ$\\boldsymbol{\\Theta} = (\\theta_1, \\theta_2, \\cdots \\theta_M)^\\top$ の関係は次のようなグラフィカルモデルで仮定します．\nこのグラフィカルモデルから同時分布を求めます． $$ \\begin{align} p(\\boldsymbol{X}, \\boldsymbol{Y}, \\boldsymbol{\\Theta}, \\sigma) \u0026amp;= p(\\boldsymbol{\\Theta})p(\\sigma)p(\\boldsymbol{Y} | \\boldsymbol{X}, \\boldsymbol{\\Theta}, \\sigma)p(\\boldsymbol{X})\\nonumber\\\\ \u0026amp;= p(\\boldsymbol{\\Theta})p(\\sigma)\\prod _{n=1}^N p(y_n | x_n, \\boldsymbol{\\Theta}, \\sigma)p(x_n) \\nonumber \\end{align} $$\n識別モデル$p(y_n | x_n, \\boldsymbol{\\Theta}, \\sigma)$について考えましょう． 識別モデルには線型回帰を仮定するので，次のような式になります． パラメータ$\\boldsymbol{\\Theta}$は$\\boldsymbol{\\Theta} = (\\theta_0, \\theta_1 \\cdots \\theta_M) \\in \\mathbb{R}^{M+1}$とします．\n$$ y_n = \\theta_0 + \\theta_1 x_n + \\theta_2 x_n^2 + \\cdots + \\theta_M x_n^M + \\varepsilon $$\nここで，$\\varepsilon$は不確実性を表す誤差で，次の正規分布に従うものとします． $$ \\varepsilon \\sim \\mathcal{N}(0, \\sigma) $$\nこの二つを合わせると，識別モデル$p(y_n | x_n, \\boldsymbol{\\Theta}, \\sigma)$は次の式で表すことができます．\n$$ p(y_n | x_n, \\boldsymbol{\\Theta}) = \\mathcal{N}(\\theta_0 + \\theta_1 x_n + \\theta_2 x_n^2 + \\cdots + \\theta_M x_n^M, \\sigma) $$\nパラメータの事前分布$p(\\boldsymbol{\\Theta})$，$p(\\sigma)$を仮定します． パラメータの事前分布$p(\\boldsymbol{\\Theta})$は平均$\\boldsymbol{0}$，共分散行列$20I$である多次元正規分布を仮定します．[2] $$ p(\\boldsymbol{\\Theta}) = \\mathcal{N}(\\boldsymbol{0}, 20I) $$\n精度パラメータ$\\sigma$に関しても事前分布を導入します．$0$から$1.0$までの一様分布と仮定します． $$ p(\\sigma) = U(0, 1.0) $$\n識別モデル，事前分布の仮定を含めると，同時分布$p(\\boldsymbol{X}, \\boldsymbol{Y}, \\boldsymbol{\\Theta})$は 次のように表せます． $$ p(\\boldsymbol{X}, \\boldsymbol{Y}, \\boldsymbol{\\Theta}, \\sigma) = \\mathcal{N}(\\boldsymbol{0}, 20I)U(0, 1.0)\\prod _{n=1}^N\\mathcal{N}(\\theta_0 + \\theta_1 x_n + \\theta_2 x_n^2 + \\cdots \\theta_M x_n^M, \\sigma)p(x_n) $$\nNumPyroでモデリングしてみましょう．行列計算に注意しながら実装していきます． ここでは，$M = 3$として3次関数により近似することを考えます．\n1 2 3 4 5 6 7 8 9  M = 3 def model(X, Y=None): theta = numpyro.sample(\u0026#34;theta\u0026#34;, dist.MultivariateNormal(0, 20 * jnp.eye(M + 1))) sigma = numpyro.sample(\u0026#34;sigma\u0026#34;, dist.Uniform(0.0, 1.0)) x_list = jnp.array([jnp.power(X, i) for i in range(0, M + 1)]).squeeze().T mu = numpyro.deterministic(\u0026#34;mu\u0026#34;, jnp.dot(x_list, theta.T)) y_pred = numpyro.sample(\u0026#34;y\u0026#34;, dist.Normal(mu, sigma), obs=Y)   パラメータの事後分布 同時分布$p(\\boldsymbol{X}, \\boldsymbol{Y}, \\boldsymbol{\\Theta})$が分かったので， ここからパラメータの事後分布を考えていきます． $$ p(\\boldsymbol{\\Theta} | \\boldsymbol{X}, \\boldsymbol{Y}, \\sigma) \\propto p(\\boldsymbol{\\Theta})p(\\sigma)\\prod _{n=1}^N p(y_n | x_n, \\boldsymbol{\\Theta})p(x_n) $$\n分母のモデルエビデンスは学習時には定数として扱うことができるので無視します． この事後分布を解析的，もしくは確率分布の近似計算を行うことによって求めます．\n先ほどNumPyroにより実装したモデルを学習しましょう．今回は，MCMC法を用います．\n1 2 3  kernel = numpyro.infer.NUTS(model) mcmc = numpyro.infer.MCMC(kernel, num_samples=1000, num_warmup=300, num_chains=4, chain_method=\u0026#34;parallel\u0026#34;) mcmc.run(jax.random.PRNGKey(0), X, Y)   MCMC法の場合，トレースプロットというものがあります．MCMC法を用いてサンプリングを行った後，トレースプロットの収束を見ることで，近似した分布が妥当かどうかを調べることができます．パラメータthetaとsigmaのトレースプロットを見てみましょう．\n1  az.plot_trace(mcmc, var_names=[\u0026#34;theta\u0026#34;, \u0026#34;sigma\u0026#34;])   トレースプロットの結果を次に示します．\nそれぞれのパラメータに対するトレースプロットは，\u0026ldquo;毛虫\u0026quot;のように収束していることがわかります．事後分布についても一定に収束していることがわかります．\n予測分布 予測分布も考えましょう．学習された$\\boldsymbol{\\Theta}$の分布$p(\\boldsymbol{\\Theta} | \\boldsymbol{X}, \\boldsymbol{Y})$を使って新しい入力値$x_\\star$に対応する 未知の出力値$y_\\star$に関する予測分布$p(y_\\star | x_\\star \\boldsymbol{X}, \\boldsymbol{Y})$を求めます． $$ p(y_\\star | x_\\star, \\boldsymbol{X}, \\boldsymbol{Y}) = \\int p(y_\\star | x_\\star, \\boldsymbol{\\Theta}, \\sigma)p(\\Theta | \\boldsymbol{X}, \\boldsymbol{Y})p(\\sigma | \\boldsymbol{X}, \\boldsymbol{Y}) d\\boldsymbol{\\Theta} $$\nNumPyroで予測分布を求めます．先ほどMCMC法により，各パラメータの事後分布を求めたので予測分布を求めることができます．\n1 2 3 4  x_ = jnp.linspace(0, 1, 100).reshape(-1, 1) posterior_samples = mcmc.get_samples() posterior_predictive = numpyro.infer.Predictive(model, posterior_samples, return_sites=[\u0026#34;y\u0026#34;]) predict = posterior_predictive(jax.random.PRNGKey(1), x_)   求めた予測分布の標本から，カーネル密度推定(kdernel density estimation)を行ってみます． カーネル密度推定は未知の確率密度関数を持つi.i.dから得られた標本から，確率変数の確率密度関数を推定する手法です．真の観測データ$y$と予測分布の標本$y_{pos}$のいくつかでカーネル密度推定を行い，真の観測データの推定した確率密度関数と予測分布の標本で推定した確率密度関数が近ければ，予測分布は妥当な推定ができると言えます．カーネル密度推定の結果を以下の図に示します．青色が真の観測データで推定した確率密度関数であり，赤色が予測分布の標本で推定した確率密度関数です．プロットを見ると，真の観測データと予測分布の標本によって推定された確率密度関数がある程度近いことがわかるので，学習して求めた予測分布は妥当な推論ができると言えます．\nでは，$xy$平面に予測分布をプロットして，どれだけ予測がうまくできているかを見てみます． $xy$平面に予測分布を90%信頼区間でプロットします．予測結果をわかりやすくするために，データセットもプロットします．濃い青の曲線は予測分布の期待値で，薄い青色で塗られている部分が予測分布の90%信頼区間です．\nこの予測分布を見ると，期待値は観測データを作る際に用いた生成過程である$y = \\sin{(2\\pi x)}$に近く，また，確率分布を導入したことによって，点推定ではなく不確実性も考慮した予測できていることがわかります．すごい．\n終わりに この記事を書くことを通して，自分が勉強していて曖昧になっていたことが明確に理解できるようになったことが多くありました．やはり，勉強したことをアウトプットしてみることは大事なことなのだと改めて思いました．今回の記事では，実装についても扱いました．わからないことを色々調べたり，何度もテストをしてみたりしながらモデルを作っていったので，理論と実装を絡めながら理解することができたのでよかったです．\n実はこの記事は前置きです．19日にもAdvent Calendarに記事を出します．今度はここで書いた知識を応用して音声合成の観点からみた隠れマルコフモデルを理解する予定です．書き終わるかわかりませんがよろしくお願いします．\n参考文献  須山敦司著，杉山将監修，『ベイズ推論による機械学習入門』 C.M.ビショップ，『パターン認識と機械学習 ベイズ推論による統計的予測』 河原創，『機械学習で楽しむJAX/NumPyro v0.1.2』   [1] グラフィカルモデルにおいて，円で書かれている変数が確率変数です．特に黒色の円は観測された確率変数であることを表します．四角形はプレートと呼ばれ，プレートの中にあるモデルが独立同分布に従った状態で複数個あることを表します． [2] 共分散行列$\\boldsymbol{\\Sigma} = \\sigma I$は対角成分が$\\sigma$，それ以外が0の行列となっているため，変量間は互いに独立です．\n ","date":"2022-12-16T00:00:00+09:00","image":"https://hiro-1219.github.io/blog/p/%E3%83%99%E3%82%A4%E3%82%BA%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92%E3%81%AE%E6%B5%81%E3%82%8C%E3%82%92%E7%90%86%E8%A7%A3%E3%81%99%E3%82%8B/img/predict_hu51fec60104b53e505959d9e24eefcf2a_39255_120x120_fill_box_smart1_3.png","permalink":"https://hiro-1219.github.io/blog/p/%E3%83%99%E3%82%A4%E3%82%BA%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92%E3%81%AE%E6%B5%81%E3%82%8C%E3%82%92%E7%90%86%E8%A7%A3%E3%81%99%E3%82%8B/","title":"ベイズ機械学習の流れを理解する"},{"content":"はじめまして、やじるしです。ブログ始めました。\n自分は、情報工学を専攻している学生です。 趣味は音楽で、ピアノ弾きます。ボーカロイドも好きです。いつか曲も書いてみたい。 興味がある分野は、人工知能（特に機械学習、深層学習）と音声処理（音声合成）、音楽情報処理です。確率統計も興味があります。これらのことについて勉強したことも書いておきたいです。\nせっかくブログ始めたならジャンル問わずいろんなことを書いていきたいと思ってます。どうぞよろしくお願いします。\n","date":"2022-07-29T00:00:00Z","permalink":"https://hiro-1219.github.io/blog/p/%E3%81%AF%E3%81%98%E3%82%81%E3%81%BE%E3%81%97%E3%81%A6/","title":"はじめまして"}]